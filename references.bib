
@article{zhang_survey_2022,
	title = {A {Survey} on {Multi}-{Task} {Learning}},
	volume = {34},
	issn = {1041-4347, 1558-2191, 2326-3865},
	url = {https://ieeexplore.ieee.org/document/9392366/},
	doi = {10.1109/TKDE.2021.3070203},
	abstract = {Multi-Task Learning (MTL) is a learning paradigm in machine learning and its aim is to leverage useful information contained in multiple related tasks to help improve the generalization performance of all the tasks. In this paper, we give a survey for MTL from the perspective of algorithmic modeling, applications, and theoretical analyses. For algorithmic modeling, we give a deﬁnition of MTL and then classify different MTL algorithms into ﬁve categories, including feature learning approach, low-rank approach, task clustering approach, task relation learning approach, and decomposition approach as well as discussing the characteristics of each approach. In order to improve the performance of learning tasks further, MTL can be combined with other learning paradigms including semi-supervised learning, active learning, unsupervised learning, reinforcement learning, multi-view learning, and graphical models. When the number of tasks is large or the data dimensionality is high, we review online, parallel, and distributed MTL models as well as dimensionality reduction and feature hashing to reveal their computational and storage advantages. Many real-world applications use MTL to boost their performance and we review representative works. Finally, we present theoretical analyses and discuss several future directions for MTL.},
	language = {en},
	number = {12},
	urldate = {2023-01-31},
	journal = {IEEE Trans. Knowl. Data Eng.},
	author = {Zhang, Yu and Yang, Qiang},
	month = dec,
	year = {2022},
	pages = {5586--5609},
	file = {Zhang y Yang - 2022 - A Survey on Multi-Task Learning.pdf:/home/papabloblo/Zotero/storage/RNAJKY2Z/Zhang y Yang - 2022 - A Survey on Multi-Task Learning.pdf:application/pdf},
}

@article{apley_visualizing_2020,
	title = {Visualizing the effects of predictor variables in black box supervised learning models},
	volume = {82},
	issn = {1369-7412, 1467-9868},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/rssb.12377},
	doi = {10.1111/rssb.12377},
	abstract = {In many supervised learning applications, understanding and visualizing the effects of the predictor variables on the predicted response is of paramount importance. A shortcoming of black box supervised learning models (e.g. complex trees, neural networks, boosted trees, random forests, nearest neighbours, local kernel-weighted methods and support vector regression) in this regard is their lack of interpretability or transparency. Partial dependence plots, which are the most popular approach for visualizing the effects of the predictors with black box supervised learning models, can produce erroneous results if the predictors are strongly correlated, because they require extrapolation of the response at predictor values that are far outside the multivariate envelope of the training data. As an alternative to partial dependence plots, we present a new visualization approach that we term accumulated local effects plots, which do not require this unreliable extrapolation with correlated predictors. Moreover, accumulated local effects plots are far less computationally expensive than partial dependence plots. We also provide an R package ALEPlot as supplementary material to implement our proposed method.},
	language = {en},
	number = {4},
	urldate = {2023-01-04},
	journal = {J. R. Stat. Soc. B},
	author = {Apley, Daniel W. and Zhu, Jingyu},
	month = sep,
	year = {2020},
	pages = {1059--1086},
	file = {Apley y Zhu - 2020 - Visualizing the effects of predictor variables in .pdf:/home/papabloblo/Zotero/storage/QQMG9GXC/Apley y Zhu - 2020 - Visualizing the effects of predictor variables in .pdf:application/pdf},
}

@article{caruana_multitask_1997,
	title = {Multitask {Learning}},
	volume = {28},
	issn = {08856125},
	url = {http://link.springer.com/10.1023/A:1007379606734},
	doi = {10.1023/A:1007379606734},
	number = {1},
	urldate = {2023-05-15},
	journal = {Machine Learning},
	author = {Caruana, Rich},
	year = {1997},
	pages = {41--75},
	file = {Texto completo:/home/papabloblo/Zotero/storage/Z3EDKV8C/Caruana - 1997 - [No title found].pdf:application/pdf},
}

@inproceedings{dosilovic_explainable_2018,
	address = {Opatija},
	title = {Explainable artificial intelligence: {A} survey},
	isbn = {978-953-233-095-3},
	shorttitle = {Explainable artificial intelligence},
	url = {https://ieeexplore.ieee.org/document/8400040/},
	doi = {10.23919/MIPRO.2018.8400040},
	urldate = {2023-11-20},
	booktitle = {2018 41st {International} {Convention} on {Information} and {Communication} {Technology}, {Electronics} and {Microelectronics} ({MIPRO})},
	publisher = {IEEE},
	author = {Dosilovic, Filip Karlo and Brcic, Mario and Hlupic, Nikica},
	month = may,
	year = {2018},
	pages = {0210--0215},
	file = {Dosilovic et al. - 2018 - Explainable artificial intelligence A survey.pdf:/home/papabloblo/Zotero/storage/7GEW2UIT/Dosilovic et al. - 2018 - Explainable artificial intelligence A survey.pdf:application/pdf},
}

@article{barredo_arrieta_explainable_2020,
	title = {Explainable {Artificial} {Intelligence} ({XAI}): {Concepts}, taxonomies, opportunities and challenges toward responsible {AI}},
	volume = {58},
	issn = {15662535},
	shorttitle = {Explainable {Artificial} {Intelligence} ({XAI})},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1566253519308103},
	doi = {10.1016/j.inffus.2019.12.012},
	language = {en},
	urldate = {2023-11-20},
	journal = {Information Fusion},
	author = {Barredo Arrieta, Alejandro and Díaz-Rodríguez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garcia, Salvador and Gil-Lopez, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
	month = jun,
	year = {2020},
	pages = {82--115},
	file = {Versión aceptada:/home/papabloblo/Zotero/storage/958PSC99/Barredo Arrieta et al. - 2020 - Explainable Artificial Intelligence (XAI) Concept.pdf:application/pdf},
}

@article{eiter_computing_1994,
	title = {Computing {Discrete} {Frechet} {Distance}},
	abstract = {The Fréchet distance between two curves in a metric space is a measure of the similarity between the curves. We present a discrete variation of this measure. It provides good approximations of the continuous measure and can be efficiently computed using a simple algorithm. We also consider variants of discrete Fréchet distance, and find an interesting connection to measuring distance between theories.},
	author = {Eiter, Thomas and Mannila, Heikki},
	month = may,
	year = {1994},
	file = {Full Text PDF:/home/papabloblo/Zotero/storage/36C2ISPY/Eiter y Mannila - 1994 - Computing Discrete Frechet Distance.pdf:application/pdf},
}

@article{azzalini_class_1985,
	title = {A {Class} of {Distributions} {Which} {Includes} the {Normal} {Ones}},
	volume = {12},
	issn = {0303-6898},
	url = {https://www.jstor.org/stable/4615982},
	abstract = {A new class of density functions depending on a shape parameter λ is introduced, such that λ=0 corresponds to the standard normal density. The properties of this class of density functions are studied.},
	number = {2},
	urldate = {2023-11-22},
	journal = {Scandinavian Journal of Statistics},
	author = {Azzalini, A.},
	year = {1985},
	note = {Publisher: [Board of the Foundation of the Scandinavian Journal of Statistics, Wiley]},
	pages = {171--178},
	file = {Azzalini - 1985 - A Class of Distributions Which Includes the Normal.pdf:/home/papabloblo/Zotero/storage/EPGERJMH/Azzalini - 1985 - A Class of Distributions Which Includes the Normal.pdf:application/pdf},
}

@article{breiman_random_2001,
	title = {Random forests},
	volume = {45},
	issn = {08856125},
	url = {http://link.springer.com/10.1023/A:1010933404324},
	doi = {10.1023/A:1010933404324},
	number = {1},
	urldate = {2023-11-29},
	journal = {Machine Learning},
	author = {Breiman, Leo},
	year = {2001},
	pages = {5--32},
	file = {Texto completo:/home/papabloblo/Zotero/storage/K3X3LQEE/Breiman - 2001 - [No title found].pdf:application/pdf},
}

@inproceedings{carroll_task_2005,
	address = {Montreal, Que., Canada},
	title = {Task similarity measures for transfer in reinforcement learning task libraries},
	volume = {2},
	isbn = {978-0-7803-9048-5},
	url = {http://ieeexplore.ieee.org/document/1555955/},
	doi = {10.1109/IJCNN.2005.1555955},
	abstract = {Recent research in task transfer and task clustering has necessitated the need for task similarity measures in reinforcement learning. Determining task similarity is necessary for selective transfer where only information from relevant tasks and portions of a task are transferred. Which task similarity measure to use is not immediately obvious. It can be shown that no single task similarity measure is uniformly superior. The optimal task similarity measure is dependent upon the task transfer method being employed. We define similarity in terms of tasks, and propose several possible task similarity measures, dT, dp, dQ, and dR which are based on the transfer time, policy overlap, Q-values, and reward structure respectively. We evaluate their performance in three separate experimental situations.},
	language = {en},
	urldate = {2023-12-05},
	booktitle = {Proceedings. 2005 {IEEE} {International} {Joint} {Conference} on {Neural} {Networks}, 2005.},
	publisher = {IEEE},
	author = {Carroll, J.L. and Seppi, K.},
	year = {2005},
	pages = {803--808},
	file = {Carroll y Seppi - 2005 - Task similarity measures for transfer in reinforce.pdf:/home/papabloblo/Zotero/storage/TFZTMJM3/Carroll y Seppi - 2005 - Task similarity measures for transfer in reinforce.pdf:application/pdf},
}

@inproceedings{parameswaran_large_2010,
	title = {Large {Margin} {Multi}-{Task} {Metric} {Learning}},
	volume = {23},
	url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/087408522c31eeb1f982bc0eaf81d35f-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Parameswaran, Shibin and Weinberger, Kilian Q},
	editor = {Lafferty, J. and Williams, C. and Shawe-Taylor, J. and Zemel, R. and Culotta, A.},
	year = {2010},
	file = {Texto completo:/home/papabloblo/Zotero/storage/5J6KFFAU/Parameswaran y Weinberger - 2010 - Large Margin Multi-Task Metric Learning.pdf:application/pdf},
}

@article{evgeniou_learning_2005,
	title = {Learning {Multiple} {Tasks} with {Kernel} {Methods}},
	volume = {6},
	url = {http://jmlr.org/papers/v6/evgeniou05a.html},
	number = {21},
	journal = {Journal of Machine Learning Research},
	author = {Evgeniou, Theodoros and Micchelli, Charles A. and Pontil, Massimiliano},
	year = {2005},
	pages = {615--637},
	file = {Texto completo:/home/papabloblo/Zotero/storage/XBRDLQHK/Evgeniou et al. - 2005 - Learning Multiple Tasks with Kernel Methods.pdf:application/pdf},
}

@article{barredo_arrieta_explainable_2020-1,
	title = {Explainable {Artificial} {Intelligence} ({XAI}): {Concepts}, taxonomies, opportunities and challenges toward responsible {AI}},
	volume = {58},
	issn = {15662535},
	shorttitle = {Explainable {Artificial} {Intelligence} ({XAI})},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1566253519308103},
	doi = {10.1016/j.inffus.2019.12.012},
	language = {en},
	urldate = {2023-12-05},
	journal = {Information Fusion},
	author = {Barredo Arrieta, Alejandro and Díaz-Rodríguez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garcia, Salvador and Gil-Lopez, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
	month = jun,
	year = {2020},
	pages = {82--115},
	file = {Versión aceptada:/home/papabloblo/Zotero/storage/V72VPTGD/Barredo Arrieta et al. - 2020 - Explainable Artificial Intelligence (XAI) Concept.pdf:application/pdf},
}

@article{friedman_greedy_2001,
	title = {Greedy {Function} {Approximation}: {A} {Gradient} {Boosting} {Machine}},
	volume = {29},
	issn = {0090-5364},
	shorttitle = {Greedy {Function} {Approximation}},
	url = {https://www.jstor.org/stable/2699986},
	abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent "boosting" paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such "TreeBoost" models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.},
	number = {5},
	urldate = {2023-12-18},
	journal = {The Annals of Statistics},
	author = {Friedman, Jerome H.},
	year = {2001},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {1189--1232},
}

@article{friedman_greedy_2001-1,
	title = {Greedy {Function} {Approximation}: {A} {Gradient} {Boosting} {Machine}},
	volume = {29},
	url = {http://www.jstor.org/stable/2699986},
	language = {en},
	number = {5},
	journal = {The Annals of Statistics},
	author = {Friedman, Jerome H.},
	year = {2001},
	pages = {1189--1232},
	file = {Friedman - 2001 - Greedy Function Approximation A Gradient Boosting.pdf:/home/papabloblo/Zotero/storage/7JGPLQVW/Friedman - 2001 - Greedy Function Approximation A Gradient Boosting.pdf:application/pdf},
}

@article{frechet_sur_1906,
	title = {Sur quelques points du calcul fonctionnel},
	volume = {22},
	issn = {0009-725X, 1973-4409},
	url = {http://link.springer.com/10.1007/BF03018603},
	doi = {10.1007/BF03018603},
	language = {it},
	number = {1},
	urldate = {2024-01-15},
	journal = {Rend. Circ. Matem. Palermo},
	author = {Fréchet, M. Maurice},
	month = dec,
	year = {1906},
	pages = {1--72},
	file = {Versión enviada:/home/papabloblo/Zotero/storage/PTCK3X4E/Fréchet - 1906 - Sur quelques points du calcul fonctionnel.pdf:application/pdf},
}

@article{kullback_information_1951,
	title = {On {Information} and {Sufficiency}},
	volume = {22},
	issn = {0003-4851},
	url = {http://projecteuclid.org/euclid.aoms/1177729694},
	doi = {10.1214/aoms/1177729694},
	language = {en},
	number = {1},
	urldate = {2024-01-15},
	journal = {Ann. Math. Statist.},
	author = {Kullback, S. and Leibler, R. A.},
	month = mar,
	year = {1951},
	pages = {79--86},
	file = {Texto completo:/home/papabloblo/Zotero/storage/2QITPQ86/Kullback y Leibler - 1951 - On Information and Sufficiency.pdf:application/pdf},
}

@book{manning_introduction_2008,
	edition = {1},
	title = {Introduction to {Information} {Retrieval}},
	isbn = {978-0-521-86571-5 978-0-511-80907-1},
	url = {https://www.cambridge.org/core/product/identifier/9780511809071/type/book},
	abstract = {Class-tested and coherent, this textbook teaches classical and web information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. It gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. Slides and additional exercises (with solutions for lecturers) are also available through the book's supporting website to help course instructors prepare their lectures.},
	urldate = {2024-01-15},
	publisher = {Cambridge University Press},
	author = {Manning, Christopher D. and Raghavan, Prabhakar and Schütze, Hinrich},
	month = jul,
	year = {2008},
	doi = {10.1017/CBO9780511809071},
	file = {Texto completo:/home/papabloblo/Zotero/storage/K4SXT9V3/Manning et al. - 2008 - Introduction to Information Retrieval.pdf:application/pdf},
}

@incollection{seel_measures_2012,
	address = {Boston, MA},
	title = {Measures of {Similarity}},
	isbn = {978-1-4419-1427-9 978-1-4419-1428-6},
	url = {http://link.springer.com/10.1007/978-1-4419-1428-6_503},
	language = {en},
	urldate = {2024-01-15},
	booktitle = {Encyclopedia of the {Sciences} of {Learning}},
	publisher = {Springer US},
	author = {Ifenthaler, Dirk},
	editor = {Seel, Norbert M.},
	year = {2012},
	doi = {10.1007/978-1-4419-1428-6_503},
	pages = {2147--2150},
}

@misc{athanasios_tsanas_parkinsons_2009,
	title = {Parkinsons {Telemonitoring}},
	url = {https://archive.ics.uci.edu/dataset/189},
	doi = {10.24432/C5ZS3N},
	urldate = {2024-01-19},
	publisher = {UCI Machine Learning Repository},
	author = {Athanasios Tsanas, Max Little},
	year = {2009},
}

@article{tsanas_accurate_2010,
	title = {Accurate {Telemonitoring} of {Parkinson}'s {Disease} {Progression} by {Noninvasive} {Speech} {Tests}},
	volume = {57},
	issn = {0018-9294},
	url = {http://ieeexplore.ieee.org/document/5339170/},
	doi = {10.1109/TBME.2009.2036000},
	number = {4},
	urldate = {2024-01-19},
	journal = {IEEE Trans. Biomed. Eng.},
	author = {Tsanas, A. and Little, M.A. and McSharry, P.E. and Ramig, L.O.},
	month = apr,
	year = {2010},
	pages = {884--893},
}
